Ambari: 
	is a console application for provisioning,managing & monitor hadoop cluster.
It consist list of REST API browser based management interface.


2 components:
a) ambari server  (master process communicates with ambari agent)
b) ambari agents (runs in all the hosts, each agent periodically sends health status & metric info).


https://www.edupristine.com/blog/hadoop-installation-using-ambari

https://www.youtube.com/watch?v=wHaVBoLwzwU


Master node IP : 192.168.13.82    easwaran/123456   :  chennai.vectone.com



192.168.11.73 chennai.vectone.com node1
192.168.11.193 india-133.squay.com node2


Terminal:
=========
a) ssh-keygen
   following files will be generated under /.ssh folder.
   i) id_rsa  (private key)
   ii) id_rsa.pub (public key)

b) create & copy the public key & give permission.
   i) cp id_rsa.pub authorized_keys 
  ii) chmod 600 authorized_keys

c) Move the 'authorized_keys' file into target linux system under root/.ssh folder.

post that target system can be accessed without prompting password.
  ex : ssh root@<IP>  


repos:
i) ambari , hdp, hdp_util

stop firewall:
systemctl stop firewall

ssh root@india-133.squay.com

ssh hadoop@india-133.squay.com

which java : command to get java home path.

https://www.youtube.com/watch?v=f9Yw-czkaKg


a) sudo wget http://public-repo-1.hortonworks.com/ambari/centos7-ppc/2.x/updates/2.6.0.0/ambari.repo -O /etc/yum.repos.d/ambari.repo

                                                                                         
b) yum repolist
c) sudo yum install -y ambari-server
d) 

sudo yum install ambari

HDP : harton works distribution of hadoop.

Slave IP : 192.168.11.193  root/mundio123  hadoop/hadoop  : india-133.squay.com

ssh chennai.vectone.com “systemctl stop firewalld”

ssh 192.168.11.193 “systemctl stop firewalld”


http://hortonworks129.rssing.com/chan-52247624/all_p2.html
==========================================================
wget http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.1.1/ambari-2.1.1-centos7.tar.gz
wget http://public-repo-1.hortonworks.com/HDP/centos7/2.x/updates/2.3.0.0/HDP-2.3.0.0-centos7-rpm.tar.gz
wget http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.20/repos/centos7/HDP-UTILS-1.1.0.20-centos7.tar.gz

export JAVA_HOME=/usr/bin/java/jdk.1.8.0_151/
export PATH=$PATH:$JAVA_HOME/bin

tar -xzvf ambari-2.1.1-centos7.tar.gz -C /var/www/html/
tar -zxvf HDP-2.3.0.0-centos7-rpm.tar.gz -C /var/www/html
tar -zxvf HDP-UTILS-1.1.0.20-centos7.tar.gz -C /var/www/html

sudo cat CentOS-Base.repo
sudo vim ambari.repo

ambari-server setup –j /usr/java/bin/default

http://hortonworks69.rssing.com/chan-30456327/all_p60.html
sudo wget http://public-repo-1.hortonworks.com/ARTIFACTS/jdk-7u67-linux-x64.tar.gz 

tar -xzvf jdk-7u67-linux-x64.tar.gz -C /usr/java

/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.161-0.b14.el7_4.x86_64

alternatives –remove jre_openjdk /usr/lib/jvm/jre-1.7.0-openjdk.x86_64/bin/java PATH TO THE OPEN JDK

alternatives –remove /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.65-3.b17.el7.x86_64/jre/bin/java
tar -xzvf jdk-7u67-linux-x64.gz -C /usr/java/

alternatives
sudo date
java installation.


export JAVA_HOME=/usr/java/jdk1.7.0_67/
export PATH=$PATH:$JAVA_HOME/bin

http://chennai.vectone.com:8080/#/login

yum list | grep ambari-agent

hbase password: hbase@123
Grafana : grafana@123

oozie : oozie@123

actvity analysis : admin@123

systemctl stop firewalld


ambari-server setup -j /usr/java/default

Pending:
=======
Services network restart
Disable the firewall

cd /etc/yum.repos.d/
for i in master slave1 slave2
Set Up the Ambari Server


-------------------------------


Admin Name : admin

Cluster Name : BigdataCluster

Total Hosts : 2 (2 new)

Repositories:

    redhat7 (HDP-2.6):
    http://public-repo-1.hortonworks.com/HDP/centos7/2.x/updates/2.6.3.0

    redhat7 (HDP-UTILS-1.1.0.21):
    http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.21/repos/centos7

Services:

    HDFS
        DataNode : 2 hosts
        NameNode : chennai.vectone.com
        NFSGateway : 0 host
        SNameNode : chennai.vectone.com
    YARN + MapReduce2
        App Timeline Server : india-133.squay.com
        NodeManager : 1 host
        ResourceManager : chennai.vectone.com
    Tez
        Clients : 2 hosts
    Hive
        Metastore : india-133.squay.com
        HiveServer2 : india-133.squay.com
        WebHCat Server : india-133.squay.com
        Database : New MySQL Database
    HBase
        Master : india-133.squay.com
        RegionServer : 1 host
        Phoenix Query Server : 1 host
    Pig
        Clients : 2 hosts
    Sqoop
        Clients : 2 hosts
    ZooKeeper
        Server : 2 hosts
    Flume
        Flume : 1 host
    Ambari Infra
        Infra Solr Instance : india-133.squay.com
    Ambari Metrics
        Metrics Collector : india-133.squay.com
        Grafana : india-133.squay.com
    Kafka
        Broker : chennai.vectone.com
    SmartSense
        Activity Analyzer : india-133.squay.com
        Activity Explorer : india-133.squay.com
        HST Server : india-133.squay.com
    Spark
        Livy Server : 0 host
        History Server : india-133.squay.com
        Thrift Server : 0 host
    Spark2
        Livy for Spark2 Server : 0 host
        History Server : india-133.squay.com
        Thrift Server : 0 host
    Slider
        Clients : 2 hosts


-----------------------------------------------------
hdfs://chennai.vectone.com:8020/apps/hbase/data
hdfs://chennai.vectone.com:8020/apps/hbase/data

Error 	Ambari Metrics 	hbase.rootdir 	file:///var/lib/ambari-metrics-collector/hbase 	

No disk info found on host india-133.squay.com

Ambari Metrics service uses HBase as default storage backend. Set the rootdir for HBase to either local filesystem path if using Ambari Metrics in embedded mode or to a HDFS dir, example: hdfs://namenode.example.org:8020/amshbase.


==============================================================================================================

Java Installation.:

cd /opt/
wget --no-cookies --no-check-certificate --header "Cookie: gpw_e24=http%3A%2F%2Fwww.oracle.com%2F; oraclelicense=accept-securebackup-cookie" "http://download.oracle.com/otn-pub/java/jdk/8u161-b12/2f38c3b165be4555a1fa6e98c45e0808/jdk-8u161-linux-x64.tar.gz"
tar xzf jdk-8u161-linux-x64.tar.gz

cd /opt/jdk1.8.0_161/
alternatives --install /usr/bin/java java /opt/jdk1.8.0_161/bin/java 2
alternatives --config java
choose option 1.

alternatives --install /usr/bin/jar jar /opt/jdk1.8.0_161/bin/jar 2
alternatives --install /usr/bin/javac javac /opt/jdk1.8.0_161/bin/javac 2
alternatives --set jar /opt/jdk1.8.0_161/bin/jar
alternatives --set javac /opt/jdk1.8.0_161/bin/javac

==============================================================================================================


----------------------------------------------------------------------

Steps:

a) Verfify the existence of java in our machine & remove unwanted jdks.
	1. alternatives -display java
	2. alternatives -remove java /usr/lib/jvm/jre-1.7.0-openjdk.x86_64/bin/java

b) Installation of Java :
   1. mkdir /usr/java
   2. URL :  http://hortonworks69.rssing.com/chan-30456327/all_p60.html
   3. Commands
	sudo wget http://public-repo-1.hortonworks.com/ARTIFACTS/jdk-7u67-linux-x64.tar.gz 
	tar -xzvf jdk-7u67-linux-x64.tar.gz -C /usr/java
c)  Class path setting.

  1. gedit /etc/profile 
  2. Update the below entries
	export JAVA_HOME=/usr/java/1.8.0_65/
	export PATH=$PATH:$JAVA_HOME/bin		

 Note: Do the same accross all the nodes.

d) Create user account.
   useradd hadoop
   passwd hadoop

e) Configuring Key based login.

    1. Master:

	ssh-keygen  (.ssh folder will be created in master along with private key (id_rsa) & public key file)
	ssh slave1.mundio.net "mkdir /root/.ssh"
	ssh slave2.mundio.net "mkdir /root/.ssh"

   cat id_rsa.pub >> authorized_keys      (copy file into authorized_keys)
   scp /root/.ssh/authorized_keys root@slave1.mundio.net://root/.ssh
   scp /root/.ssh/authorized_keys root@slave2.mundio.net://root/.ssh

scp /root/.ssh/authorized_keys root@192.168.13.85://root/.ssh

  2. Slave1 (node1)
	 ssh-keygen
	 cd .ssh
	 cat id_rsa.pub >> authorized_keys

	scp /root/.ssh/authorized_keys root@master.mundio.net://root/.ssh
	scp /root/.ssh/authorized_keys root@slave2.mundio.net://root/.ssh

Note: Repeat the steps accross all the nodes & system should allow to access without prompting passoword.
ex:
  ssh root@node1.com
  ssh root@node2.com

f) Add host name in host files.

   execute the below command in all the nodes to get the host names.

 command:   hostname

 vi /etc/hosts  (add the below entries)

	192.168.13.82 chennai.vectone.com node1
	192.168.11.193 india-133.squay.com master

h) Setting Up a Local Repository  (doubt)

	Let’s install httpd to configure the web repositories

	yum search httpd
	yum install httpd
	
	If you get an error please add below entry in centos /etc/yum.repos.d/
	gpgcheck =0

  & to Services network restart
	service httpd status
	service httpd start

i) Disable the firewall

	Run the below command on all the nodes

	systemctl stop firewall
	ssh master “systemctl stop firewalld”

i1) create hadoop user & generate key & add it with authorized_keys of (root)

j) Download the Ambari Repository

	1. http://hortonworks129.rssing.com/chan-52247624/all_p2.html

	wget http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.1.1/ambari-2.1.1-centos7.tar.gz
	wget http://public-repo-1.hortonworks.com/HDP/centos7/2.x/updates/2.3.0.0/HDP-2.3.0.0-centos7-rpm.tar.gz
	wget http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.20/repos/centos7/HDP-UTILS-1.1.0.20-centos7.tar.gz

	2. Extract the downloaded tar.gz files:
		sudo tar -xzvf ambari-2.1.1-centos7.tar.gz -C /var/www/html/
		sudo tar -xzvf HDP-2.3.0.0-centos7-rpm.tar.gz -C /var/www/html/
		sudo tar -xzvf HDP-UTILS-1.1.0.20-centos7.tar.gz -C /var/www/html/
		
k) Preparing the Ambari Repository Configuration File

wget -nv http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.6.0.0/ambari.repo -O /etc/yum.repos.d/ambari.repo

	cd /etc/yum.repos.d/
	download ambari.repo file & place here

l) Installing Ambari.
	1. Start ambari installation
          yum install ambari-server

	2. Set selinux to disabled in all the nodes
		vi /etc/selinux/config
		Change the SELINUX=enforcing to disabled

SELINUX=disabled

============================================

	for i in master slave1 slave2 slave3 slave4 ; do ssh $i "selinux";done
	Repeat the above step in all the nodes
	======================================

m) Set Up the Ambari Server
	Setup configures Ambari to talk to the Ambari database, installs the JDK and allows you to customize the user
	account the Ambari Server daemon will run as.
	For Java:

	ambari-server setup -j /opt/jdk1.8.0_161

	For postgres
	ambari-server setup -jdbc-db=postgres --jdbc-driver=/usr/share/java/postgresql-jdbc.jar

	


n) Start the Ambari Server
	1. Run the following command on the Ambari Server host:
		ambari-server start
	2. To check the Ambari Server processes:
		ambari-server status
	3. To stop the Ambari Server:
		ambari-server stop 

o) Log In to Apache Ambari
	After starting the Ambari service, open Ambari Web using a web browser.
		http://india-133.squay.com:8080/#/login


http://master.mundio.com:8080/#/login
http://india-133.squay.com:8080/#/login

	Note: Log in to the Ambari Server using the default user name/password: admin/admin.

ambari server logs:
cat /var/log/ambari-server/ambari-server.log

p) Launching the Ambari Install Wizard

q) Passwords:
    1. Hive 
     database password : database@123

   2. Ambari Metrics 
      Grafana Admin : grafana@123

   3. Smart sense
      Activity Analysis : admin@123


Admin Name : admin

Cluster Name : DevCluster

Total Hosts : 1 (1 new)

Repositories:

    redhat7 (HDP-2.6):
    http://public-repo-1.hortonworks.com/HDP/centos7/2.x/updates/2.6.3.0

    redhat7 (HDP-UTILS-1.1.0.21):
    http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.21/repos/centos7

Services:

    HDFS
        DataNode : 1 host
        NameNode : india-133.squay.com
        NFSGateway : 0 host
        SNameNode : india-133.squay.com
    YARN + MapReduce2
        App Timeline Server : india-133.squay.com
        NodeManager : 1 host
        ResourceManager : india-133.squay.com
    Tez
        Clients : 1 host
    Hive
        Metastore : india-133.squay.com
        HiveServer2 : india-133.squay.com
        WebHCat Server : india-133.squay.com
        Database : New MySQL Database
    HBase
        Master : india-133.squay.com
        RegionServer : 1 host
        Phoenix Query Server : 0 host
    Pig
        Clients : 1 host
    Sqoop
        Clients : 1 host
    ZooKeeper
        Server : india-133.squay.com
    Flume
        Flume : 1 host
    Ambari Infra
        Infra Solr Instance : india-133.squay.com
    Ambari Metrics
        Metrics Collector : india-133.squay.com
        Grafana : india-133.squay.com
    Kafka
        Broker : india-133.squay.com
    SmartSense
        Activity Analyzer : india-133.squay.com
        Activity Explorer : india-133.squay.com
        HST Server : india-133.squay.com
    Spark
        Livy Server : 0 host
        History Server : india-133.squay.com
        Thrift Server : 0 host
    Spark2
        Livy for Spark2 Server : 0 host
        History Server : india-133.squay.com
        Thrift Server : 0 host
    Slider
        Clients : 1 host

=====================================================================

Summary :
=========

The cluster consists of 1 hosts

    Installed and started services successfully on 1 new host

Master services installed

    NameNode installed on india-133.squay.com
    SNameNode installed on india-133.squay.com
    ResourceManager installed on india-133.squay.com
    History Server installed on india-133.squay.com
    HiveServer2 installed on india-133.squay.com
    HBase Master installed on india-133.squay.com

All services started

All tests passed

Install and start completed in 79 minutes and 58 seconds



From the Ambari Welcome page, choose Launch Install Wizard
Name Your Cluster: BigdataCluster
The UI displays repository Base URLs based on Operating System Family (OS Family). Be sure to set the correct
OS Family based on the Operating System you are running.
Change the repositories. Get them from hdp.repo. Enter the list of hostnames (Fully qualified domain names)
and then copy the /root/.ssh/id_rsa in the ambari-server machine
Confirm Hosts
Confirm Hosts prompts you to confirm that Ambari has located the correct hosts for your cluster and to check
those hosts to make sure they have the correct directories, packages, and processes required to continue the
install
Choose Services
You may choose to install any other available services now. The install wizard selects all available services for
installation by default.
1. Choose none to clear all selections, or choose all to select all listed services.
2. Choose or clear individual checkboxes to define a set of services to install now.
3. After selecting the services to install now, choose Next.
Assign Masters
The Ambari install wizard assigns the master components for selected services to appropriate hosts in
your cluster and displays the assignments in Assign Masters. The left column shows services and current
hosts. The right column shows current master component assignments by host, indicating the number of
CPU cores and amount of RAM installed on each host.
1. To change the host assignment for a service, select a host name from the drop-down menu for that
service.
2. To remove a ZooKeeper instance, click the green minus icon next to the host address you want to
remove.
3. When you are satisfied with the assignments, choose Next

--------------------------------------------------------------------------------

kafka --> spark --> rule engine

http://india-133.squay.com:6667
https://www.youtube.com/watch?v=uIAeGufvSXA

------------------------------------------------------------------------------------------------------

Class path setting:


export ZOOKEEPER_HOME=/usr/hdp/2.6.3.0-235/zookeeper


#SPARK ECOSYSTEMS
export KAFKA_HOME=/usr/hdp/2.6.3.0-235/kafka

------------------------------------------------------------------------------------------------------------

Topic Creation:

cd /bin
cat kafka
export KAFKA_HOME=/usr/hdp/2.6.3.0-235/kafka
export ZOOKEEPER_HOME=/usr/hdp/2.6.3.0-235/zookeeper

Starting Zookeeper:
/usr/hdp/2.6.3.0-235/kafka/bin/kafka-server-start.sh /usr/hdp/2.6.3.0-235/kafka/config/server.properties 

Starting Zookeeper Client in 2181 port:
/usr/hdp/2.6.3.0-235/zookeeper/bin/zkCli.sh start india-133.squay.com:2181


Creating Topics:

/usr/hdp/2.6.3.0-235/kafka/bin/kafka-topics.sh --create --zookeeper india-133.squay.com:2181 --replication-factor 1  --partitions 1 --topic eventtopic

/usr/hdp/2.6.3.0-235/kafka/bin/kafka-topics.sh --create --zookeeper india-133.squay.com:2181 --replication-factor 1  --partitions 1 --topic bundlenotify


/usr/hdp/2.6.3.0-235/kafka/bin/kafka-topics.sh --zookeeper india-133.squay.com:2181 --delete --topic bundlenotify hgpo.llo.prmt.processed 


/usr/hdp/2.6.3.0-235/kafka/bin/kafka-topics.sh --zookeeper india-133.squay.com:2181 --disable --topic bundlenotify

List Topics:
/usr/hdp/2.6.3.0-235/kafka/bin/kafka-topics.sh --list --zookeeper india-133.squay.com:2181


start producer:
/usr/hdp/2.6.3.0-235/kafka/bin/kafka-console-producer.sh --broker-list india-133.squay.com:6667 --topic eventtopic

send message from producer.

start event topic consumer :
/usr/hdp/2.6.3.0-235/kafka/bin/kafka-console-consumer.sh --zookeeper india-133.squay.com:2181 --topic eventtopic --from-beginning   
consumer will recieve message from producer.
	
/usr/hdp/2.6.3.0-235/kafka/bin/kafka-console-consumer.sh --zookeeper india-133.squay.com:2181 --topic bundlenotify --from-beginning   
consumer will recieve message from producer.


Start Consumers:

/usr/hdp/2.6.3.0-235/kafka/bin/kafka-console-consumer.sh --zookeeper india-133.squay.com:2181 --topic eventtopic --from-beginning

/usr/hdp/2.6.3.0-235/kafka/bin/kafka-console-consumer.sh --zookeeper india-133.squay.com:2181 --topic bundlenotify --from-beginning	  

------------------------------------------------------------------------------------------------------------------------

Kafka Producer URL :
http://192.168.12.59:8511/producer/send


	{
  "topic":"eventtopic",
  "key":"bundle_customer",
  "message":"{\"bundleID\":12,\"actionType\":\"subscribebundle\",\"status\":1,\"mobileNo\":420794099805,\"mvbpStatus\":1,\"updateBy\":\"USSD\",\"currCode\":\"CZK\"}"
}

Change content type in REST Client:
Header --> custom Header
name : Content-Type
Value : application/json
URL : http://192.168.11.193:8511/producer/send

method : Post.

------------------------------------------------------------------------------------------------------------------------
Maven:

export M2_HOME=/home/hadoop/Easwar/Soft/maven
export PATH=${M2_HOME}/bin:${PATH}

/home/hadoop/IdeaProjects/target/kafkaproducer-1.0-SNAPSHOT.jar

scp target/kafkaproducer-1.0-SNAPSHOT.jar hadoop@192.168.11.193:Soft/KafkaProducer

------------------------------------------------------------------------------------------------------------------------

Run Spark :

/bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master spark://207.184.161.138:7077 \
  --executor-memory 20G \
  --total-executor-cores 100 \
  /path/to/examples.jar \
  1000


scp MarketingAutomation-assembly-1.0.jar hadoop@192.168.11.193:Soft/MarketingAutomation

/bin/spark-submit --class com.mundio.mat.Driver --master yarn-client --deploy-mode client --num-executors 2 --driver-memory 1G --executor-memory 1G --executor-cores 1 Soft/MarketingAutomation/MarketingAutomation-assembly-1.0.jar 

With Hbase:
 /bin/spark-submit --class com.mundio.mat.Driver --master yarn-client --deploy-mode client --conf "spark.executor.extraClassPath=/usr/hdp/current/hbase-client/conf/:/usr/hdp/current/phoenix-client/phoenix-4.7.0.2.5.0.0-1245-client.jar" --num-executors 2 --driver-memory 1G --executor-memory 1G --executor-cores 1 Soft/MarketingAutomation/MarketingAutomation-assembly-1.0.jar 



Issue : Multiple versions of Spark are installed but SPARK_MAJOR_VERSION is not set
Spark1 will be picked by default

Soln : export SPARK_MAJOR_VERSION=2

	/application_1522837242746_0008":hdfs:hdfs:drwxr-xr-x

Soln : 
sudo -u hdfs hadoop fs -mkdir /user/hadoop/.sparkStaging
export HADOOP_USER_NAME=hdfs

--------------------------------------------------------------------------------------------------------------------------

Pending :
b) Foreach (spark)
c) size / capacity configuration.
d) .xml configuration.
h) class path setting.
i) connectivity between 2 cluster components (spark , hbase)
j) configuration part / file path.
k) change scala code to java.
l) Y hbase is been used instead hive.
j) Hive (sql)
k) adding new node in the cluster (components to be selected)
l) flume (messaging system)

                  
+-------------+------------+------------+----------------------------+---------+
| 34492       | Frantisek  | Dvorak     | francescodvorak@gmail.com  |         |
| 999         | Milena     | Klatilova  | mrichterova@gmail.com      |      


Adding new node with Cluster:
============================
a) Add host entry in /etc/hosts
b) SSH key generation
c) installed ntp (network Time Protocol) --> keep the same time in all the nodes.
d) 

HOSTNAME

192.168.12.59 india-133.squay.com master 

========================================================================================

version : 2.5.2

1. Repository:
    is a space where it host the s/w packages which can be used for download & install.

2. Yum : is a package manager, which actually fetches the s/w  packages from the repository.

3. Local repository :  hosted space in local environment. when the machine dont have active internet connection , the local repository will be setup & the user will be able to obtain ambari & other hadoop s/w packages from local.

4. Types of Ambari repostories: 
    a) Ambari
    b) Hadoop
    c) Hadoop utils
    d) EPEL (Extra packages for Enterprise Linux)

5. Tools needed to build Ambari
   a) JDK
   b) Maven 3.3.9/later

6. Life cycle of Ambari
   Install, start , install , configure , stop

7. 

---------------------------------------------------------------------------

Ambari :
=========



 



   




